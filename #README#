#############################################################################################
##################################### WHAT IS DIIM ##########################################
#############################################################################################

DIIM is a general framework, tested for the accurate estimation of marine optical constituents
from satellite-derived Remote Sensing Reflectance (Rrs). As part of the New Copernicus
Capability for Trophic Ocean Networks (NECCTON, https://neccton.eu/) project, we aim to use
DIIM to improve the current data assimilation with Rrs data.

DOI: https://doi.org/10.5194/gmd-2024-174.

#############################################################################################
######################################## REQUIREMENTS #######################################
#############################################################################################

The scripts for this work were made in python3, tested in python=3.8.9 and python=3.9.6.
For the automatic installation, is required to have a Darwin of Linux prompt, as well as
pip to install the requirements. All the libraryes required are listed in the file
requirements.txt.


#############################################################################################
##################################### HOW TO INSTALL DIIM ###################################
#############################################################################################

A makefile has been tested for python=3.8.9 and python3=3.9.6. It creates a virtual environment
with all the python3 libraries and adds in $SHRC=.bashrc or .zshrc the path to diim, as well as to the
diim enviroment. To create it, run

     $ make
     $ source $HOME/$SHRC
     $ source $/path/to/diim/environment/bin/activate

If for some reason, the make file is not an option, e.g. you are using conda to install the
dependencies, you have to make sure all required libraries are installed, as listed in the
requirements.txt file. You also have to set a global variable as:

	$ export DIIM_PATH=/path/to/DIIM

If desired, you can create a local Python 3 environment with conda or pip by running:

	$ python3 -m venv "./diim_env"
	$ diim_env/bin/pip install networkx==2.8.8
	$ diim_env/bin/pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
	$ diim_env/bin/pip install -r requirements.txt

	      



#############################################################################################
##################################### INSTALLING DIIM ###################################
#############################################################################################

To run the codes, make sure all required libraries are installed, as listed in the
requirements.txt file. You can also set a global library path for diim by running:

	$ export DIIM_PATH=/path/to/DIIM

If desired, you can create a local Python 3 environment with conda or pip by running:

	$ python3 -m venv "./diim_env"
	$ diim_env/bin/pip install networkx==2.8.8
	$ diim_env/bin/pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
	$ diim_env/bin/pip install -r requirements.txt



#############################################################################################
##################################### DATA AVAILABILITY #####################################
#############################################################################################
All the data used in our work is contained in settings/npy_data. For easy accessibility,
the data is stored in binary files in NumPy .npy format. To access the data, read it with a Python script like:

$ python3
$ >>> import numpy
$ >>> file = numpy.load('settings/npy_data/x_data_all.npy')

The data will then be in a matrix, where each row represents a different date of data, and each column
represents a different input. A description of each column is available in settings/npy_data/README.

#############################################################################################
##################################### MODULES DESCRIPTION ###################################
######################################### (diimpy) ##########################################
#############################################################################################

The modules in diimpy contain all the scripts used for this work:

    read_data_module.py: All scripts use PyTorch and NumPy for handling the data. This module
    contains classes and functions to read the data and transform it into formats that are
    easier to work with in conjunction with PyTorch tensors.

    Forward_model.py: The work deals with an inverse process. This module contains most of the
    forward computations.

    bayesian_inversion.py: For the inversion computation, many approaches were tested. This
    module contains functions to perform Bayesian inversion, along with alternate minimization.

    sensitivity_analysis_and_mcmc_runs.py: This script is used to study the sensitivity
    analysis of the parameters of the model, run the MCMC, and generate plots of the
    sensitivity. The MCMC uses the output of the alternate minimization (computed in the
    bayesian_inversion.py module) as input.

    CVAE_model_part_one.py / CVAE_model_part_two.py: These scripts are used to train the neural
    network-based inversion model. Since this is the most complex part, I will explain how it
    works in more detail. There are two main functions in the CVAE_model_part_two module:

        explore_hyperparameters(): This function uses the ray[Tune] module to search the hyperparameter
   	space for those that minimize a loss function on the validation set. The validation set is 10%
	of the training data, selected randomly for each iteration. ray[Tune] uses the Bayesian
	Optimization HyperBand algorithm for efficient hyperparameter search. The hyperparameters
	explored and the corresponding loss values for each iteration are stored in a file called
	ray_tune, located by default in $HOME/ray_tune. You can read this file using functions from
	the ray[Tune] module and select the one with the best score.

        save_cvae_first_part(): After hyperparameter exploration, this function reads the best
	hyperparameters and trains the neural network with all the training data. Our model is
	composed of three parts: two neural networks stored in settings/VAE_model and a forward
	model (diimpy/ForwardModel) of RRS. The first part, model_first_part.pt (NN_1), was trained
	with OASIM and Satellite data as input and BOUSSOLE data as output. The goal of this part
	was to reduce the dimensionality of the input. The second part, model_second_part_final.pt
	(NN_2), was trained in CVAE_model_part_two.py using NN_1 as input and producing latent
	variables (e.g., chla, nap, cdom) as output. Since this is a probabilistic model, using the
	mean value from multiple runs is recommended.

    	In the folder /settings/VAE_model, there is a second file, model_second_part_chla_centered.pt,
    	which can be used as NN_2 instead of model_second_part_final.pt. This model has the same
    	hyperparameters, but its output for chla is the sum of NN_1_chla (output from NN_1 mapping the
 	input to in-situ chla data) and NN_2_chla (a perturbation to NN_1_chla). Using this model
	results in lower RMSE for chla values in the Test data when using the forward model RRS.
	To choose whether to use chla_centered or not, set chla_centered=True/False in the NN_second_layer class.

    plot_data_lognormal.py: Code used to create all the plots in this work, except for the
    sensitivity analysis plots.

    ModelOnePointFive.py: While exploring the best approach for the inversion, the code went
    through many iterations. ModelOnePointFive (model 1.5) is the fifth iteration of the inversion
    process, incorporating vectorized quantities, parallel computing, lognormal uncertainties
    (originally Gaussian noise), etc. This is the final version used to compute alternate minimization
    and Bayesian inversion. The modules bayesian_inversion.py and read_data_module.py are modular
    copies of this code, with additional upgrades for compatibility with future work.

#############################################################################################
##################################### SETTINGS ##############################################
#############################################################################################

The folder settings contains all the necessary files to run the codes in diimpy:

    npy_data: As described in the 'DATA AVAILABILITY' section, this folder contains the training
    and test data used in this work.

    Jacobians: Contains the derivatives of the functions bbp, kd, and RRS with respect to the
    parameters "perturbation_factors" for all available historical data.

    VAE_model: Contains the state_dict of the NN models. The final model used is
    model_second_part_chla_centered.pt, which uses model_first_part.pt.

    perturbation_factors: The parameters of the model were perturbed by multiplying them by
    perturbation factors, which were optimized using alternate minimization, MCMC, and the
    SGVB framework (CVAE files).

    cte_lambda.csv / cte.csv: Literature values were used as initial conditions for the model
    parameters. The lambda-dependent parameters are in cte_lambda.csv, while the
    lambda-independent parameters are in cte.csv.


#############################################################################################
################################# HOW TO TEST AND REPRODUCE #################################
#############################################################################################

A simple test script called test.py has been added to check if everything is working correctly
 and also serves as a guide for how the code works (read the comments in the script).

For a longer and detailed description on how to reproduce the results in the paper, run the 
script reproduce.py, by runing:

>>>python3 reproduce.py [-h|-a|-b|-m|-n]

Use -h for help, -a for all, -b for only the bayes part, -m for only the mcmc part and -n for 
only the neural network part.
