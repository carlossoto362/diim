#############################################################################################
##################################### WHAT IS DIIM ##########################################
#############################################################################################

DIIM is a general framework, tested for the accurate estimation of marine optical constituents
from satellite-derived Remote Sensing Reflectance (Rrs). As part of the New Copernicus
Capability for Trophic Ocean Networks (NECCTON, https://neccton.eu/) project, we aim to use
DIIM to improve the current data assimilation with Rrs data.

DOI: https://doi.org/10.5194/gmd-2024-174.

#############################################################################################
######################################## REQUIREMENTS #######################################
#############################################################################################

The scripts for this work were written in Python 3 and tested in Python >= 3.8.
For automatic installation, a Darwin or Linux prompt with pip installed is required.
 All the libraries required are listed in the next section.

#############################################################################################
##################################### HOW TO INSTALL DIIM ###################################
#############################################################################################

A Makefile has been tested with Python 3.8.9, Python 3.9.6, and Python 3.12.7.
It creates a virtual environment with all the Python 3 libraries and adds the path to diim,
as well as the diim environment, to $SHRC (.bashrc or .zshrc):
     $ make
     $ source $HOME/$SHRC
     $ source $/path/to/diim/environment/bin/activate

To add the python version you want to use, add

     $make PYTHON_VERSION=python3.8

If, for some reason, the Makefile is not an option (e.g., you are using conda to install
the dependencies), you must ensure that all required libraries are installed, as listed
in the requirements.txt file, along with the libraries networkx >= 2.8.8, torch, torchvision,
and torchaudio. You also need to set a global variable as:

	$ export DIIM_PATH=/path/to/DIIM

For completeness, I list here the libraries required for all the scripts to work:

    networkx>=2.8.8
    torch
    torchvision
    torchaudio
    ConfigSpace>=1.1.1
    constrained-linear-regression>=0.0.4
    matplotlib>=3.7
    matplotlib-inline>=0.1.7
    mdurl>=0.1.2
    multiprocess>=0.70.16
    networkx>=2.8.8
    numpy>=1.24
    pandas>=2
    ray>=2.1
    scipy>=1.1
    seaborn>=0.13.2
    sympy>=1.12
    pvlib>=0.11.0
    tqdm>=4.66.5
    pyarrow
    tensorboardX

#############################################################################################
##################################### DATA AVAILABILITY #####################################
#############################################################################################
All the data used in our work is contained in settings/npy_data. For easy accessibility,
the data is stored in binary files in NumPy .npy format. To access the data, read it with a Python script like:

$ python3
$ >>> import numpy
$ >>> file = numpy.load('settings/npy_data/x_data_all.npy')

The data will then be in a matrix, where each row represents a different date of data, and each column
represents a different input. A description of each column is available in settings/npy_data/README.

#############################################################################################
##################################### MODULES DESCRIPTION ###################################
######################################### (diimpy) ##########################################
#############################################################################################

The modules in diimpy contain all the scripts used for this work:

    read_data_module.py: All scripts use PyTorch and NumPy for handling the data. This module
    contains classes and functions to read the data and transform it into formats that are
    easier to work with in conjunction with PyTorch tensors.

    Forward_model.py: The work deals with an inverse process. This module contains most of the
    forward computations.

    bayesian_inversion.py: For the inversion computation, many approaches were tested. This
    module contains functions to perform Bayesian inversion, along with alternate minimization.

    sensitivity_analysis_and_mcmc_runs.py: This script is used to study the sensitivity
    analysis of the parameters of the model, run the MCMC, and generate plots of the
    sensitivity. The MCMC uses the output of the alternate minimization (computed in the
    bayesian_inversion.py module) as input.

    CVAE_model_part_one.py / CVAE_model_part_two.py: These scripts are used to train the neural
    network-based inversion model. Since this is the most complex part, I will explain how it
    works in more detail. There are two main functions in the CVAE_model_part_two module:

        explore_hyperparameters(): This function uses the ray[Tune] module to search the hyperparameter
   	space for those that minimize a loss function on the validation set. The validation set is 10%
	of the training data, selected randomly for each iteration. ray[Tune] uses the Bayesian
	Optimization HyperBand algorithm for efficient hyperparameter search. The hyperparameters
	explored and the corresponding loss values for each iteration are stored in a file called
	ray_tune, located by default in $HOME/ray_tune. You can read this file using functions from
	the ray[Tune] module and select the one with the best score.

        save_cvae_first_part(): After hyperparameter exploration, this function reads the best
	hyperparameters and trains the neural network with all the training data. Our model is
	composed of three parts: two neural networks stored in settings/VAE_model and a forward
	model (diimpy/ForwardModel) of RRS. The first part, model_first_part.pt (NN_1), was trained
	with OASIM and Satellite data as input and BOUSSOLE data as output. The goal of this part
	was to reduce the dimensionality of the input. The second part, model_second_part_final.pt
	(NN_2), was trained in CVAE_model_part_two.py using NN_1 as input and producing latent
	variables (e.g., chla, nap, cdom) as output. Since this is a probabilistic model, using the
	mean value from multiple runs is recommended.

    	In the folder /settings/VAE_model, there is a second file, model_second_part_chla_centered.pt,
    	which can be used as NN_2 instead of model_second_part_final.pt. This model has the same
    	hyperparameters, but its output for chla is the sum of NN_1_chla (output from NN_1 mapping the
 	input to in-situ chla data) and NN_2_chla (a perturbation to NN_1_chla). Using this model
	results in lower RMSE for chla values in the Test data when using the forward model RRS.
	To choose whether to use chla_centered or not, set chla_centered=True/False in the NN_second_layer class.

    plot_data_lognormal.py: Code used to create all the plots in this work, except for the
    sensitivity analysis plots.

    ModelOnePointFive.py: While exploring the best approach for the inversion, the code went
    through many iterations. ModelOnePointFive (model 1.5) is the fifth iteration of the inversion
    process, incorporating vectorized quantities, parallel computing, lognormal uncertainties
    (originally Gaussian noise), etc. This is the final version used to compute alternate minimization
    and Bayesian inversion. The modules bayesian_inversion.py and read_data_module.py are modular
    copies of this code, with additional upgrades for compatibility with future work.

#############################################################################################
##################################### SETTINGS ##############################################
#############################################################################################

The folder settings contains all the necessary files to run the codes in diimpy:

    npy_data: As described in the 'DATA AVAILABILITY' section, this folder contains the training
    and test data used in this work.

    Jacobians: Contains the derivatives of the functions bbp, kd, and RRS with respect to the
    parameters "perturbation_factors" for all available historical data.

    VAE_model: Contains the state_dict of the NN models. The final model used is
    model_second_part_chla_centered.pt, which uses model_first_part.pt.

    perturbation_factors: The parameters of the model were perturbed by multiplying them by
    perturbation factors, which were optimized using alternate minimization, MCMC, and the
    SGVB framework (CVAE files).

    cte_lambda.csv / cte.csv: Literature values were used as initial conditions for the model
    parameters. The lambda-dependent parameters are in cte_lambda.csv, while the
    lambda-independent parameters are in cte.csv.


#############################################################################################
################################# HOW TO TEST AND REPRODUCE #################################
#############################################################################################

A simple test script called test.py has been added to check if everything is working correctly
 and also serves as a guide for how the code works (read the comments in the script).

For a longer and detailed description on how to reproduce the results in the paper, run the 
script reproduce.py, by runing:

>>>python3 reproduce.py [-h|-a|-b|-m|-n]

Use -h for help, -a for all, -b for only the bayes part, -m for only the mcmc part and -n for 
only the neural network part.
