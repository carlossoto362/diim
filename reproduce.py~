import argparse
import sys
import os
import time
import numpy as np
import torch
from torch.utils.data import DataLoader,random_split
from torch import nn
from tqdm import tqdm

if 'DIIM_PATH' in os.environ:
    HOME_PATH = MODEL_HOME = os.environ["DIIM_PATH"]
else:
    
    print("Missing local variable DIIM_PATH. \nPlease add it with '$:export DIIM_PATH=path/to/diim'.")
    sys.exit()

def argument():
    parser = argparse.ArgumentParser(description = '''
    Script to reproduce the results in DOI: https://doi.org/10.5194/gmd-2024-174.
    ''',
                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter
                                     )
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument( '-i',
                         action='store_true',
                         help='Interactive script, use it to have a gided tour on how to reproduce the results.'
    )
    group.add_argument('-b',
                        action='store_true',
                        help='Only bayesian part'
    group.add_argument('-m',
                       action='store_true',
                       help='Only sensitivity analysis and mcmc part'
    )
                                                 
    return parser.parse_args()

def iprint(message,delay = 0.02):
    """Prints a message letter by letter, with a given delay between letters."""
    print("Interactive cat: - ",end='')
    
    for letter in message:
        sys.stdout.write(letter)  # Print letter without newline
        sys.stdout.flush()         # Ensure it appears immediately
        time.sleep(delay)          # Wait before printing the next letter
    print()  # Newline after the message is printed
    print('')
    
def cprint(message,output=None):
    print('>>>',end='')
    print(message)
    if output:
        print(output)
        
def iflag(flag=None):
    limit_tr = 0
    while limit_tr < 3:
        flag = input('User - ')
        flag = ("".join(flag.strip())).lower()
        if flag in ['yes','no']:
            return flag
        else:
            iprint('Please wirte yes or no.')
        limit_tr += 1
    if k == 3:
                       sys.exit()

def bayesian_part():
        
    file_ = open(HOME_PATH + '/settings/reproduce/interactive_cat')
    interactive_cat = file_.read()
    print(interactive_cat)
    iprint('Welcome to the interactive script to reproduce the results from DOI: https://doi.org/10.5194/gmd-2024-174. At any time type ctl+c to stop the interactive experience')
        try:
        
            iprint('The paper "Data-Informed Inversion Model (DIIM):  framework to retrieve marine optical constituents in the BOUSSOLE site using a three-stream irradiance model" use a One dimensional bio-optical model and hystorical data to test two different approaches to retrieve marine optical constituents from Remote Sensing Reflectance data. The intention is to retrieve values with uncertainties, and also, to optimize the model using data from past measurements.',delay = 0.002)
        
            iprint('The first approach we tested was the Bayesian approach. As descrived in the README file (please read it), and explaind it in the test.py file (also read it, is meant to be read, not just to run it), the scripts needed to reproduce this part are in the diimpy, and are: 1) read_data_module.py, used to read the data and transform the input in torch tensors, 2)Forward_module.py, which contain the forward computations. All the formulas needed to compute the theoretical Remote Sensing Reflectance as a funciton of the OASIM model inputs, the optical constituents, and the empirical parameters from the model. 3) bayesian_inversion.py, this module has the scripts needed to do the inversion (stand allone), or in combination with the alternate minimization (by optimizing the parameters simultaneusly).',delay = 0.002)

            cprint('from diimpy import read_data_module as rdm')
            cprint('from diimpy import Forward_module as fm')
            cprint('from diimpy iport bayesian_inversion as bayes')
            print('')

            from diimpy import read_data_module as rdm
            from diimpy import Forward_module as fm
            from diimpy import bayesian_inversion as bayes



            iprint('To recreate the paper, we first run alternate minimization, with an approximately uniform prior (alpha >>1)')
            iprint('Would you like to run this part? (yes/no)')
            flag=iflag()
            if flag == 'yes':
                iprint('This part will save the history of perturbation_factors_used in DIIM_PATH + "/settings/reproduce/perturbation_factors/perturbation_factors_history_new.pt"')
                cprint("track_parameters(data_path = MODEL_HOME + '/settings/npy_data',output_path = MODEL_HOME + '/settings/reproduce/perturbation_factors',iterations=1000,save=True )")
                bayes.track_parameters(data_path = MODEL_HOME + '/settings/npy_data',output_path = MODEL_HOME + '/settings/reproduce/perturbation_factors',iterations=1000,save=True )
            
            print('')

            
            iprint('With this perturbation factors, we tuned the prior parameter alpha (See the paper, appendix B). This step is needed since each day, the inversion is performed with only five wavelenghts. With so little data, the prior has a big influence in the uncertainty, our choice of how to tune alpha is symilar to the bayesian model specification, where if we assume gaussianity, the best model is the one that has a balance between fitting the data, and lower the individual uncertainties.')
            iprint('Would you like to run this part? (yes/no)')
            flag=iflag()
            if flag == 'yes':
                iprint('This will save the output of the bayesian minimization with the different alphas in DIIM_PATH + "/settings/reproduce/alphas", for this step we used as perturbation factors the output of the previous step, but the one saved in DIIM_PATH + "/settings/perturbation_factors/perturbation_factors_history_AM_test.npy".')
                cprint("track_alphas(output_path = MODEL_HOME + '/settings/reproduce/alphas',save=True)")
                print('')
                iprint('This can take a bit of time...')
                bayes.track_alphas(output_path = MODEL_HOME + '/settings/reproduce/alphas',save=True)

            
            iprint('Finally, we can run the inversion, with the best alpha, and the optimal perturbation factors, to obtain the historical optical constituents plus uncertainty.')
            iprint('Would you like to run this part? (yes/no)')
            flag=iflag()
            if flag == 'yes':
                iprint('This may take a bit of time...')
                print(""">>>data = rdm.customTensorData(data_path=MODEL_HOME + '/settings/npy_data',which='all',per_day = True,randomice=False)
                >>>perturbation_factors = torch.tensor(np.load(MODEL_HOME + '/settings/perturbation_factors/perturbation_factors_history_AM_test.npy'))[-1].to(torch.float32)

    
                >>>my_device = 'cpu' # the forward computations are not optimal to run with cuda
                >>>constant = rdm.read_constants(file1=MODEL_HOME + '/settings/cte_lambda.csv',file2=MODEL_HOME + '/settings/cte.csv',my_device = my_device)

                >>>lr = 0.029853826189179603 #tuned by runing with many lr's
                >>>x_a = torch.zeros(3)
                >>>s_a_ = torch.eye(3)
                >>>s_e = (torch.eye(5)*torch.tensor([1.5e-3,1.2e-3,1e-3,8.6e-4,5.7e-4]))**(2)#validation rmse from https://catalogue.marine.copernicus.eu/documents/QUID/CMEMS-OC-QUID-009-141to144-151to154.pdf
                >>>batch_size = data.len_data
                >>>dataloader = DataLoader(data, batch_size=batch_size, shuffle=False)
                >>>s_a = s_a_*4.9 #this is the best alpha 
                
                >>>model = fm.Forward_Model(num_days=batch_size).to(my_device)
                >>>model.perturbation_factors = perturbation_factors
                >>>loss = fm.RRS_loss(x_a,s_a,s_e,num_days=batch_size,my_device = my_device)
                >>>optimizer = torch.optim.Adam(model.parameters(),lr=lr)
                
                >>>output = bayes.train_loop(next(iter(dataloader)),model,loss,optimizer,4000,kind='all',\
                num_days=batch_size,constant = constant,perturbation_factors_ = perturbation_factors, scheduler = True)
                
                >>>output_path = MODEL_HOME+'/settings/reproduce/results_AM'
                >>>np.save(output_path + '/X_hat.npy',output['X_hat'])
                >>>np.save(output_path + '/kd_hat.npy',output['kd_hat'])
                >>>np.save(output_path + '/bbp_hat.npy',output['bbp_hat'])
                >>>np.save(output_path + '/RRS_hat.npy',output['RRS_hat'])
                >>>np.save(output_path + '/dates.npy',data.dates) """)
                
                data = rdm.customTensorData(data_path=MODEL_HOME + '/settings/npy_data',which='all',per_day = True,randomice=False)
                perturbation_factors = torch.tensor(np.load(MODEL_HOME + '/settings/perturbation_factors/perturbation_factors_history_AM_test.npy'))[-1].to(torch.float32)

                
                my_device = 'cpu'
                constant = rdm.read_constants(file1=MODEL_HOME + '/settings/cte_lambda.csv',file2=MODEL_HOME + '/settings/cte.csv',my_device = my_device)
                
                lr = 0.029853826189179603
                x_a = torch.zeros(3)
                s_a_ = torch.eye(3)
                s_e = (torch.eye(5)*torch.tensor([1.5e-3,1.2e-3,1e-3,8.6e-4,5.7e-4]))**(2)#validation rmse from https://catalogue.marine.copernicus.eu/documents/QUID/CMEMS-OC-QUID-009-141to144-151to154.pdf
                batch_size = data.len_data
                dataloader = DataLoader(data, batch_size=batch_size, shuffle=False)
                s_a = s_a_*4.9

                model = fm.Forward_Model(num_days=batch_size).to(my_device)
                model.perturbation_factors = perturbation_factors
                loss = fm.RRS_loss(x_a,s_a,s_e,num_days=batch_size,my_device = my_device)
                optimizer = torch.optim.Adam(model.parameters(),lr=lr)
                
                output = bayes.train_loop(next(iter(dataloader)),model,loss,optimizer,4000,kind='all',\
                                    num_days=batch_size,constant = constant,perturbation_factors_ = perturbation_factors, scheduler = True)
                
                output_path = MODEL_HOME+'/settings/reproduce/results_AM'
                np.save(output_path + '/X_hat.npy',output['X_hat'])
                np.save(output_path + '/kd_hat.npy',output['kd_hat'])
                np.save(output_path + '/bbp_hat.npy',output['bbp_hat'])
                np.save(output_path + '/RRS_hat.npy',output['RRS_hat'])
                np.save(output_path + '/dates.npy',data.dates)

                       
    
if __name__ ==  '__main__':
    args = argument()
    if args.i:
                
            print(interactive_cat)
            iprint('Nice, we have our first result, but we will not use this one. We would like to have a measure of the uncertainty of the perturbation factors also. To do so, we will use a montecarlo algorithm. Script in file sensitivity_analysis_and_mcmc_runs.py')
            cprint('from diimpy import sensitivity_analysis_and_mcmc_runs as mcmc')
            print('')
            
            from diimpy import sensitivity_analysis_and_mcmc_runs as mcmc
            
            iprint("First, let's make a sensitivity  analysis of the parameters of the model. For this end, I computed the Jacobian of the parameters with respect of the different funcitons kd, bbp and RRS. We will not do so here, since is already stored, but in case u want to do it again, the code would be >>>jacobian_rrs,jacobian_kd,jacobian_bbp = mcmc.compute_jacobians(model,X,mu_z,perturbation_factors,constant=constant), where model if the Forward model, X is the input data for the model, and mu_z is chla, nap and cdom. This function will compute the pointwais jacobians at the point 'perturbation_factors'. ")
            
            iprint('So, we read the Jacobians, and the pointwais evaluation of the functions, and plot a box plot witht the sensitivity analysis')
            print('''
            >>>data_dir = MODEL_HOME + '/settings/npy_data'

            >>>my_device = 'cpu'
    
            >>>constant = rdm.read_constants(file1=data_dir + '/../cte_lambda.csv',file2=data_dir+'/../cte.csv',my_device = my_device)
            >>>data = rdm.customTensorData(data_path=data_dir,which='all',per_day = True,randomice=False,one_dimensional = False,seed = 1853,device=my_device)
            >>>dataloader = DataLoader(data, batch_size=len(data.x_data), shuffle=False)

            >>>X,Y = next(iter(dataloader))

            >>>jacobian_rrs = np.abs(np.load(MODEL_HOME + '/settings/Jacobians/jacobian_rrs_initialparam_lognormalresults.npy')) #drrs/depsiloni
            >>>jacobian_kd = np.abs(np.load(MODEL_HOME + '/settings/Jacobians/jacobian_kd_initialparam_lognormalresults.npy'))
            >>>jacobian_bbp = np.abs(np.load(MODEL_HOME + '/settings/Jacobians/jacobian_bbp_initialparam_lognormalresults.npy'))
            
            >>>chla_hat = np.load( MODEL_HOME + '/experiments/results_bayes_lognormal_unperturbed/X_hat.npy')
            >>>kd_hat = np.load(MODEL_HOME + '/experiments/results_bayes_lognormal_unperturbed/kd_hat.npy')[:,[0,2,4,6,8]]
            >>>bbp_hat = np.load(MODEL_HOME + '/experiments/results_bayes_lognormal_unperturbed/bbp_hat.npy')[:,[0,2,4]]
            >>>rrs_hat = np.load(MODEL_HOME + '/experiments/results_bayes_lognormal_unperturbed/RRS_hat.npy')
            
            >>>perturbation_factors = torch.ones(14)
            >>>sensitivity_boxplot(jacobian_rrs,jacobian_kd,jacobian_bbp,rrs_hat,kd_hat,bbp_hat,perturbation_factors,X,\
                            title='Sensitivity of the parameters with the literature values')


            ''')
            data_dir = MODEL_HOME + '/settings/npy_data'

            my_device = 'cpu'
    
            constant = rdm.read_constants(file1=data_dir + '/../cte_lambda.csv',file2=data_dir+'/../cte.csv',my_device = my_device)
            data = rdm.customTensorData(data_path=data_dir,which='all',per_day = True,randomice=False,one_dimensional = False,seed = 1853,device=my_device)
            dataloader = DataLoader(data, batch_size=len(data.x_data), shuffle=False)

            X,Y = next(iter(dataloader))
            jacobian_rrs = np.abs(np.load(MODEL_HOME + '/settings/Jacobians/jacobian_rrs_initialparam_lognormalresults.npy')) #drrs/depsiloni
            jacobian_kd = np.abs(np.load(MODEL_HOME + '/settings/Jacobians/jacobian_kd_initialparam_lognormalresults.npy'))
            jacobian_bbp = np.abs(np.load(MODEL_HOME + '/settings/Jacobians/jacobian_bbp_initialparam_lognormalresults.npy'))
            
            chla_hat = np.load( MODEL_HOME + '/experiments/results_bayes_lognormal_unperturbed/X_hat.npy')
            kd_hat = np.load(MODEL_HOME + '/experiments/results_bayes_lognormal_unperturbed/kd_hat.npy')[:,[0,2,4,6,8]]
            bbp_hat = np.load(MODEL_HOME + '/experiments/results_bayes_lognormal_unperturbed/bbp_hat.npy')[:,[0,2,4]]
            rrs_hat = np.load(MODEL_HOME + '/experiments/results_bayes_lognormal_unperturbed/RRS_hat.npy')
            
            perturbation_factors = torch.ones(14)
            mcmc.sensitivity_boxplot(jacobian_rrs,jacobian_kd,jacobian_bbp,rrs_hat,kd_hat,bbp_hat,perturbation_factors,X,\
                                title='Sensitivity of the parameters with the literature values')
            

            
            


            
        except KeyboardInterrupt:
            print('')
            print('Are you sure you want to stop this experience :( (yes/no)')
            flag=iflag()
            if flag == 'yes':
                print(':(')
                sys.exit()
            else:
                print('^.^')
                sys.exit()
        

    elif args.b:
        iprint("why don't you want the interactive format? :(",delay=0.08)
        
